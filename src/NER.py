import json
import os
import glob
import pandas as pd
from underthesea import ner
from rapidfuzz import process, fuzz
from tqdm import tqdm
from collections import Counter
from multiprocessing import Pool, cpu_count
import warnings
warnings.filterwarnings('ignore')

# --- C·∫§U H√åNH ---
INPUT_DIR = "data/interim"      
OUTPUT_DIR = "data/NER_processed"     
MAP_FILE = "data/ticker_map.json" 
NUM_WORKERS = max(1, cpu_count() - 1)  # S·ªë CPU cores - 1

os.makedirs(OUTPUT_DIR, exist_ok=True)

# 1. Load Ticker Map
def load_ticker_map():
    if not os.path.exists(MAP_FILE):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y {MAP_FILE}. H√£y ch·∫°y src/build_ticker_map.py tr∆∞·ªõc.")
        return {}
    with open(MAP_FILE, 'r', encoding='utf-8') as f:
        raw = json.load(f)
        # Gi·ªØ nguy√™n case g·ªëc cho fuzzy matching
        return raw

TICKER_MAP = load_ticker_map()
print(f"Loaded {len(TICKER_MAP)} mappings from ticker_map.json")

# 2. H√†m tr√≠ch xu·∫•t entities t·ª´ text (d√πng NER)
def extract_companies(text):
    if not text: 
        return []
    
    try:
        tokens = ner(text)
    except:
        return []

    entities = []
    current_entity = []
    
    # Blacklist: T·ª´ qu√° chung chung, kh√¥ng ph·∫£i t√™n c√¥ng ty
    blacklist = {
        'vi·ªát nam', 'h√† n·ªôi', 'h·ªì ch√≠ minh', 'tp.hcm', 's√†i g√≤n',
        'ho√†n', 'ph√≠', 'link', 'ng√≥ng', 'ott', 'casa', 'vi·ªác',
        'big', 'top', 'vn-index', 'vnindex', 'hnx', 'upcom'
    }
    
    noise_keywords = ['ng√†y', 'th√°ng', 'nƒÉm', 'qu√Ω', 'm·ª©c', 't·ª∑ l·ªá', 'cu·ªëi', 'ƒë·∫ßu', 'n·ª≠a', 'c·ªôt m·ªëc']
    
    def is_valid_entity(text):
        """Ki·ªÉm tra xem entity c√≥ h·ª£p l·ªá kh√¥ng"""
        if len(text) < 3:
            return False
        text_lower = text.lower()
        # Lo·∫°i noise keywords
        if any(kw in text_lower for kw in noise_keywords):
            return False
        # Lo·∫°i to√†n s·ªë
        if text.replace(' ', '').replace('/', '').replace('-', '').replace('.', '').replace(',', '').isdigit():
            return False
        # Lo·∫°i blacklist
        if text_lower.strip() in blacklist:
            return False
        return True
    
    for token in tokens:
        word, pos_tag, chunk_tag, ner_tag = token
        
        # Ch·ªâ ch·∫•p nh·∫≠n ORG, LOC, PER (c√≥ th·ªÉ l√† t√™n c√¥ng ty)
        if ner_tag in ['B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-PER', 'I-PER']:
            current_entity.append(word)
        else:
            if current_entity:
                entity_text = " ".join(current_entity)
                if is_valid_entity(entity_text):
                    entities.append(entity_text)
                current_entity = []
    
    # X·ª≠ l√Ω entity cu·ªëi c√πng n·∫øu c√≤n s√≥t
    if current_entity:
        entity_text = " ".join(current_entity)
        if is_valid_entity(entity_text):
            entities.append(entity_text)
    
    return entities

# 3. H√†m Map entities sang Tickers d√πng Fuzzy Matching
def map_to_tickers(entities, target_ticker, threshold=90, debug=False):
    """
    Map c√°c entities ƒë√£ tr√≠ch xu·∫•t sang ticker symbols
    
    Args:
        entities: Danh s√°ch t√™n entities t·ª´ NER
        target_ticker: Ticker c·ªßa b√†i b√°o ƒëang x·ª≠ l√Ω (ƒë·ªÉ lo·∫°i tr·ª´)
        threshold: Ng∆∞·ª°ng fuzzy matching (0-100), m·∫∑c ƒë·ªãnh 90
    
    Returns:
        List c√°c tickers li√™n quan (kh√¥ng bao g·ªìm target_ticker)
    """
    found_tickers = set()
    
    # Aliases cho c√°c ng√¢n h√†ng/c√¥ng ty l·ªõn (t√™n vi·∫øt t·∫Øt -> ticker)
    aliases = {
        # Ng√¢n h√†ng Big 4
        'bidv': 'BID',
        'vietinbank': 'CTG',
        'vietcombank': 'VCB',
        'vcb': 'VCB',
        'agribank': 'None',  # Kh√¥ng ni√™m y·∫øt
        
        # Ng√¢n h√†ng t∆∞ nh√¢n l·ªõn
        'techcombank': 'TCB',
        'mbbank': 'MBB',
        'mb': 'MBB',
        'vpbank': 'VPB',
        'acb': 'ACB',
        '√° ch√¢u': 'ACB',
        'sacombank': 'STB',
        's√†i g√≤n th∆∞∆°ng t√≠n': 'STB',
        'stb': 'STB',
        'vib': 'VIB',
        'qu·ªëc t·∫ø': 'VIB',
        'tpbank': 'TPB',
        'ti√™n phong': 'TPB',
        'hdbank': 'HDB',
        'ph√°t tri·ªÉn tp.hcm': 'HDB',
        'msb': 'MSB',
        'h√†ng h·∫£i': 'MSB',
        'lpb': 'LPB',
        'b∆∞u ƒëi·ªán li√™n vi·ªát': 'LPB',
        'li√™n vi·ªát': 'LPB',
        'seabank': 'SSB',
        'ƒë√¥ng nam √°': 'SSB',
        'ssb': 'SSB',
        'shb': 'SHB',
        's√†i g√≤n - h√† n·ªôi': 'SHB',
        'eximbank': 'EIB',
        'xu·∫•t nh·∫≠p kh·∫©u': 'EIB',
        'eib': 'EIB',
        'ocb': 'OCB',
        'ph∆∞∆°ng ƒë√¥ng': 'OCB',
        'vietcapitalbank': 'BVB',
        'b·∫£n vi·ªát': 'BVB',
        'bvb': 'BVB',
        'vietbank': 'VBB',
        'vi·ªát nam th∆∞∆°ng t√≠n': 'VBB',
        'vbb': 'VBB',
        'abbank': 'ABB',
        'an b√¨nh': 'ABB',
        'ncb': 'NVB',
        'qu·ªëc d√¢n': 'NVB',
        'navibank': 'NVB',
        'pvcombank': 'PVB',
        'ƒë·∫°i ch√∫ng': 'PVB',
        'pgbank': 'PGB',
        'xƒÉng d·∫ßu petrolimex': 'PGB',
        'kienlongbank': 'KLB',
        'ki√™n long': 'KLB',
        'klb': 'KLB',
        'baovietbank': 'BVB',
        'b·∫£o vi·ªát': 'BVB',
        'vietabank': 'VAB',
        'vi·ªát √°': 'VAB',
        'oceanbank': 'None',  # ƒê√£ s√°p nh·∫≠p v√†o VPBank
        'gpbank': 'GPB',
        'd·∫ßu kh√≠ to√†n c·∫ßu': 'GPB',
        
        # C√¥ng ty ch·ª©ng kho√°n
        'ssi': 'SSI',
        'ch·ª©ng kho√°n s√†i g√≤n': 'SSI',
        'vci': 'VCI',
        'vietcap': 'VCI',
        'vcbs': 'None',  # Ch·ª©ng kho√°n Vietcombank, kh√¥ng ni√™m y·∫øt
        'bsc': 'BVS',
        'bidv securities': 'BVS',
        'hsc': 'HCM',
        'th√†nh ph·ªë h·ªì ch√≠ minh': 'HCM',
        'vps': 'VPS',
        'vndirect': 'VND',
        'vds': 'VDS',
        'fpts': 'FTS',
        'fpt securities': 'FTS',
        'bsi': 'BSI',
        'agriseco': 'AGR',
    }
    
    # L·∫•y danh s√°ch t√™n c√¥ng ty t·ª´ map
    company_names = list(TICKER_MAP.keys())
    all_tickers = set(TICKER_MAP.values())
    
    for entity in entities:
        entity_lower = entity.lower().strip()
        
        # C√°ch 0: Ki·ªÉm tra aliases tr∆∞·ªõc
        if entity_lower in aliases:
            ticker = aliases[entity_lower]
            # B·ªè qua n·∫øu ticker l√† None ho·∫∑c 'None' (kh√¥ng ni√™m y·∫øt)
            if ticker and ticker != 'None' and ticker != target_ticker:
                found_tickers.add(ticker)
                if debug:
                    print(f"    '{entity}' -> ALIAS {ticker}")
            continue
        
        # C√°ch 1: Exact match
        matched = False
        for comp_name, ticker in TICKER_MAP.items():
            if entity_lower == comp_name.lower():
                if ticker != target_ticker:
                    found_tickers.add(ticker)
                    if debug:
                        print(f"    '{entity}' -> '{comp_name}' (EXACT) -> {ticker}")
                matched = True
                break
        
        if matched:
            continue
            
        # C√°ch 2: Substring match (ch·ªâ v·ªõi t√™n d√†i)
        if len(entity) >= 10:
            for comp_name, ticker in TICKER_MAP.items():
                if len(comp_name) >= 15:
                    if comp_name.lower() in entity_lower or entity_lower in comp_name.lower():
                        if ticker != target_ticker:
                            found_tickers.add(ticker)
                            if debug:
                                print(f"    '{entity}' -> '{comp_name}' (SUBSTRING) -> {ticker}")
                            matched = True
                            break
        
        if matched:
            continue
        
        # C√°ch 3: Fuzzy matching
        match = process.extractOne(entity, company_names, scorer=fuzz.token_sort_ratio)
        
        if match:
            best_match_name, score, _ = match
            if debug and score >= 70:
                print(f"    '{entity}' -> '{best_match_name}' (fuzzy: {score:.1f})")
            if score >= threshold:
                ticker = TICKER_MAP[best_match_name]
                if ticker != target_ticker:
                    found_tickers.add(ticker)
        
        # C√°ch 4: Ki·ªÉm tra ticker tr·ª±c ti·∫øp
        if entity.upper() in all_tickers:
            ticker = entity.upper()
            if ticker != target_ticker:
                found_tickers.add(ticker)
                if debug:
                    print(f"    '{entity}' -> TICKER {ticker}")
                    
    return list(found_tickers)

# 4. H√†m x·ª≠ l√Ω m·ªôt b√†i b√°o (worker function cho multiprocessing)
def process_single_article(args):
    """X·ª≠ l√Ω m·ªôt b√†i b√°o - ch·∫°y song song"""
    article, ticker_target = args
    
    full_text = f"{article.get('title', '')}. {article.get('content', '')}"
    
    # Tr√≠ch xu·∫•t entities
    extracted_entities = extract_companies(full_text)
    
    # Map sang tickers
    related_tickers = map_to_tickers(extracted_entities, ticker_target, debug=False)
    
    # L·ªçc b·ªè 'None'
    related_tickers = [t for t in related_tickers if t and t != 'None']
    
    article['related_tickers'] = ",".join(related_tickers)
    return article, related_tickers

# 5. H√†m x·ª≠ l√Ω file
def process_file(filepath):
    filename = os.path.basename(filepath)
    # File input d·∫°ng: VIC_clean.json -> l·∫•y VIC
    ticker_target = filename.split('_')[0]
    
    print(f"\nüöÄ Processing: {ticker_target}")
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            articles = json.load(f)
    except Exception as e:
        print(f"  ‚ùå L·ªói ƒë·ªçc file: {e}")
        return
        
    if not articles:
        print(f"  ‚ö†Ô∏è File r·ªóng, b·ªè qua.")
        return
    
    print(f"  üìä Total articles: {len(articles)}")
    print(f"  üíª Using {NUM_WORKERS} CPU workers...")
        
    final_data = []
    related_counter = Counter()
    
    # Chu·∫©n b·ªã args cho multiprocessing
    args_list = [(article, ticker_target) for article in articles]
    
    # X·ª≠ l√Ω song song v·ªõi multiprocessing Pool
    with Pool(processes=NUM_WORKERS) as pool:
        results = list(tqdm(
            pool.imap(process_single_article, args_list, chunksize=20),
            total=len(articles),
            desc=f"Processing {ticker_target}",
            unit="article"
        ))
    
    # Thu th·∫≠p k·∫øt qu·∫£
    for article, related_tickers in results:
        final_data.append(article)
        related_counter.update(related_tickers)
    
    print(f"\n  ‚úÖ Unique tickers found: {len(related_counter)}")
        
    df = pd.DataFrame(final_data)
    if 'date' in df.columns:
        df = df.sort_values(by='date')
        
    try:
        out_csv = os.path.join(OUTPUT_DIR, f"{ticker_target}_final.csv")
        df.to_csv(out_csv, index=False, encoding='utf-8-sig')
        print(f"  ‚úÖ Saved: {out_csv}")
    except Exception as e:
        print(f"  ‚ùå L·ªói l∆∞u CSV: {e}")
    
    # L∆∞u Top Related
    top_10 = [t[0] for t in related_counter.most_common(10)]
    
    try:
        out_json = os.path.join(OUTPUT_DIR, f"{ticker_target}_relations.json")
        with open(out_json, 'w', encoding='utf-8') as f:
            json.dump({
                "target": ticker_target,
                "top_related": top_10,
                "stats": dict(related_counter.most_common(20))
            }, f, indent=4, ensure_ascii=False)
        print(f"  ‚úÖ Top related: {top_10}\n")
    except Exception as e:
        print(f"  ‚ùå L·ªói l∆∞u JSON: {e}\n")

if __name__ == "__main__":
    print(f"Loaded {len(TICKER_MAP)} mappings from ticker_map.json")
    print(f"üöÄ Using {NUM_WORKERS} CPU workers for parallel processing\n")
    
    files = glob.glob(os.path.join(INPUT_DIR, "*_clean.json"))
    
    if not files:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu n√†o trong {INPUT_DIR}.")
    else:
        print(f"üìÅ Found {len(files)} files to process.\n")
        for f in files:
            try:
                process_file(f)
            except Exception as e:
                print(f"‚ùå L·ªói x·ª≠ l√Ω file {f}: {e}")
                import traceback
                traceback.print_exc()